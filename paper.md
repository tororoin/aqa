# Less is More: Pretrain a Strong Text Encoder for Dense Passage Retrieval using a Weak Decoder

## Abstract

*I'll write this the last*

## 1. Introduction

*I'll write this the last*

## 2. Preliminaries
*i believe this will take about 2 pages*

1. **Autoencoders**
*give an Intuition of an autoencoder, how it works and its training, explaining how encoding,  decoding and training takes place*

2. **Transformers as Autoencoders**
*Draw parallels and basically show how the transformer architecture is basically an autoencoder*

3. **Any cool stuff i think that will be necessary and related to understanding the model better**

## 3. Relevant work
*i dont know what all to add over here, as the paper looked mainly at the* ```OPTIMUS``` *model and improved upon it*

## 4. The model

*explain the model and how it works and why it works*

## 5. Analysis of the results
*explain the results, idk if its needed or not. maybe discuss the datasets used and describe the metrics*

## 6. Contrast with parallel work and future work
*basically reading related papers from semantic scholar*

---

*I am open to feedback regarding the structure of the paper. Please do let me know if you feel I shoud add some more sections to the above outline for better understanding.*

---